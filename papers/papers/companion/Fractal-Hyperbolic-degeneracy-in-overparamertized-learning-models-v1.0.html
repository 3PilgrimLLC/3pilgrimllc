<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Fractal–Hyperbolic Degeneracy in Overparameterized Learning
Manifolds</title>

  <meta name="description" content="Companion explainer to
Fractal–Hyperbolic Degeneracy in Overparameterized Learning Manifolds —
a three-primitive framework unifying flat minima, fractal roughness,
hyperbolic curvature, low intrinsic dimension, and low-rank Fisher
spectra via gradient erosion, Fisher friction, and degeneracy
amplification.">
  <meta name="keywords" content="gradient erosion, Fisher friction
field, degeneracy amplification, flat minima, fractal roughness,
hyperbolic curvature, low intrinsic dimension, low-rank Fisher spectra,
negative space carving, overparameterized manifolds, phase transitions,
grokking">

  <link rel="canonical" href="https://3pilgrim.com/papers/papers/companion/Fractal-Hyperbolic-degeneracy-in-overparamertized-learning-models-v1.0.html">

  <meta property="og:site_name" content="3 Pilgrim LLC">
  <meta property="og:type" content="article">
  <meta property="og:title" content="Fractal–Hyperbolic Degeneracy in
Overparameterized Learning Manifolds — Companion Explainer">
  <meta property="og:description" content="A companion explainer for
Fractal–Hyperbolic Degeneracy in Overparameterized Learning Manifolds:
gradient erosion carves negative space, Fisher defines parametric
friction, and overparameterization amplifies degeneracy to explain key
empirical paradoxes.">
  <meta property="og:url" content="https://3pilgrim.com/papers/papers/companion/Fractal-Hyperbolic-degeneracy-in-overparamertized-learning-models-v1.0.html">
  <meta property="og:image" content="https://3pilgrim.com/assets/logo.png">
  <meta property="og:image:alt" content="3 Pilgrim LLC Logo">

  <meta name="twitter:card" content="summary_large_image">
  <meta name="twitter:title" content="Fractal–Hyperbolic Degeneracy in
Overparameterized Learning Manifolds — Companion Explainer">
  <meta name="twitter:description" content="A companion explainer for
Fractal–Hyperbolic Degeneracy in Overparameterized Learning Manifolds:
gradient erosion carves negative space, Fisher defines parametric
friction, and overparameterization amplifies degeneracy to explain key
empirical paradoxes.">
  <meta name="twitter:image" content="https://3pilgrim.com/assets/logo.png">

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "3 Pilgrim LLC",
    "url": "https://3pilgrim.com/",
    "logo": "https://3pilgrim.com/assets/logo.png",
    "description": "Independent research organization exploring decision theory, probabilistic geometry, behavioral finance, market cognition, and complex systems.",
    "foundingDate": "2013",
    "sameAs": ["https://x.com/3PilgrimLLC"]
  }
  </script>

  <script type="application/ld+json">
  {
    "@context": "https://schema.org",
    "@type": "ScholarlyArticle",
    "name": "Fractal–Hyperbolic Degeneracy in Overparameterized Learning
Manifolds — Companion Explainer",
    "author": {
      "@type": "Organization",
      "name": "3 Pilgrim LLC"
    },
    "description": "Companion explainer to Fractal–Hyperbolic Degeneracy
in Overparameterized Learning Manifolds — a three-primitive framework
unifying flat minima, fractal roughness, hyperbolic curvature, low
intrinsic dimension, and low-rank Fisher spectra via gradient erosion,
Fisher friction, and degeneracy amplification.",
    "datePublished": "February 2026",
    "version": "1.0",
    "identifier": {
      "@type": "PropertyValue",
      "propertyID": "DOI",
      "value": "10.5281/zenodo.18489279"
    },
    "url": "https://3pilgrim.com/papers/papers/companion/Fractal-Hyperbolic-degeneracy-in-overparamertized-learning-models-v1.0.html",
    "isPartOf": {
      "@type": "CreativeWorkSeries",
      "name": "3 Pilgrim Research Library"
    },
    "about": gradient erosionnegative space carvingFisher friction
fielddegeneracy amplificationflat minimafractal roughnesshyperbolic
curvaturelow intrinsic dimensionlow-rank Fisher spectraoverparameterized
manifoldsthree-phase training protocoltarget acquisition,
    "citation": "Fractal–Hyperbolic Degeneracy in Overparameterized
Learning Manifolds — DOI 10.5281/zenodo.18489279",
    "relatedLink": [
      "https://doi.org/10.5281/zenodo.18489279",
      "https://3pilgrim.com/papers-pdfs/Fractal-Hyperbolic-degeneracy-in-overparamertized-learning-models-v1.0-v1.0.pdf"
    ],
    "associatedMedia": {
      "@type": "MediaObject",
      "contentUrl": "https://3pilgrim.com/papers-pdfs/Fractal-Hyperbolic-degeneracy-in-overparamertized-learning-models-v1.0-v1.0.pdf",
      "encodingFormat": "application/pdf"
    }
  }
  </script>

  <style>
    body {
      margin: 0;
      padding: 0;
      font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif;
      background: #f8f9fa;
      color: #222;
      line-height: 1.65;
    }
    .content-container {
      max-width: 800px;
      margin: 40px auto;
      padding: 40px 30px;
      background: white;
      box-shadow: 0 4px 20px rgba(0,0,0,0.08);
      border-radius: 8px;
      overflow-y: auto;
      max-height: 92vh;
    }
    @media (max-width: 600px) {
      .content-container {
        margin: 20px;
        padding: 20px 15px;
        border-radius: 0;
      }
    }
    .title-page {
      max-width: 800px;
      margin: 20% auto 0 auto;
      padding: 0 30px;
      text-align: center;
      page-break-after: always;
    }
    .title-page h1, .title-page h2, .title-page p {
      margin: 0.5em 0;
    }
    header {
      position: fixed;
      top: 1cm;
      width: 100%;
      text-align: center;
      font-size: 10pt;
      color: gray;
    }
    footer {
      position: fixed;
      bottom: 1cm;
      width: 100%;
      text-align: center;
      font-size: 10pt;
      color: gray;
    }
    img { max-width: 100%; height: auto; }
    h1, h2, h3 { color: #111; margin: 1.5em 0 0.8em; }
    p { margin: 1.2em 0; }
    hr { border: 0; border-top: 1px solid #ddd; margin: 2.5em 0; }
  </style>

  </head>
<body>
  <header>
    3 Pilgrim LLC | Fractal–Hyperbolic Degeneracy in Overparameterized
Learning Manifolds | Version 1.0 · February 5, 2026
  </header>

  <div class="title-page">
    <h1>Fractal–Hyperbolic Degeneracy in Overparameterized Learning
Manifolds</h1>
    <h2>A Companion Explainer</h2>
    <p>3 Pilgrim LLC</p>
    <p>Version 1.0 · February 5, 2026</p>
  </div>

  <div class="content-container">
      <p><a href="https://doi.org/10.5281/zenodo.18489279">Click here for full PDF of paper</a></p>  
	<hr />
      <p><strong>1) Why This Paper Exists</strong></p>
      <p>Modern overparameterized neural networks exhibit a
      constellation of strange, widely observed behaviors: flat minima,
      hyperbolic curvature, fractally rough loss boundaries, low
      intrinsic dimension, and low‑rank Fisher spectra. These
      observations are real but fragmented in the literature. There is
      no minimal theory explaining <em>why</em> these features co‑occur
      or how they jointly enable efficient training at scale. This paper
      proposes that all of them arise from three simple primitives
      governing the geometry of overparameterized optimization. The goal
      is to unify disparate empirical results into a <em>causal</em> and
      <em>structural</em> account, and to derive a practical three‑phase
      training protocol from that structure. <span
      data-custom-style="Hyperlink"></span></p>
      <hr />
      <p><strong>2) What the Paper Says (Plain‑Language
      Summary)</strong></p>
      <p>The paper introduces three primitives that, together, explain
      the entire observed regime of paradoxes in large‑scale
      learning:</p>
      <ol type="1">
      <li><p><strong>Gradient Erosion (Negative‑Space
      Carving).</strong><br />
      Training removes redundant directions instead of filling the
      space. Erosion collapses vast degenerate regions into a resistant
      low‑dimensional core—producing flat minima, low intrinsic
      dimension, and the observed fractal roughness of boundaries. <span
      data-custom-style="Hyperlink"></span></p></li>
      <li><p><strong>Fisher Metric as a Parametric Friction
      Field.</strong><br />
      The Fisher information defines local friction: high eigenvalues =
      tight data constraints; low eigenvalues = sloppy directions. This
      explains low‑rank Fisher structure and why curvature spectra show
      hyperbolic traits. <span
      data-custom-style="Hyperlink"></span></p></li>
      <li><p><strong>Overparameterization as Degeneracy
      Amplifier.</strong><br />
      Extra parameters create vast families of nearly equivalent
      solutions. Degeneracy is not a bug but a feature: it accelerates
      exploration and enables erosion to find the stable manifold. <span
      data-custom-style="Hyperlink"></span></p></li>
      </ol>
      <p>From these primitives, the paper derives the Target Acquisition
      Protocol, a three‑phase training method exploiting evolving
      manifold geometry:</p>
      <ul>
      <li><p><strong>Phase 1: Acquisition (High‑LR).</strong> Rapid
      erosion of low‑friction directions.</p></li>
      <li><p><strong>Phase 2: Dope Re‑Ask (Mid‑Refinement).</strong>
      Re‑calibrate the optimizer once the manifold shape
      changes.</p></li>
      <li><p><strong>Phase 3: Execution (Low‑LR).</strong> Fine‑tune in
      flat valleys for generalization. <span
      data-custom-style="Hyperlink"></span></p></li>
      </ul>
      <p>Across toy experiments and a small transformer, this protocol
      yields 15–35% fewer training steps to matched accuracy under equal
      compute. <span data-custom-style="Hyperlink"></span></p>
      <hr />
      <p><strong>3) What Distinguishes This Framework</strong></p>
      <ul>
      <li><p><strong>Minimalism with explanatory reach.</strong><br />
      Only three primitives are needed to unify fractal roughness,
      hyperbolic curvature, low intrinsic dimension, low‑rank Fisher
      structure, and flat minima. No other framework connects them this
      succinctly. <span data-custom-style="Hyperlink"></span></p></li>
      <li><p><strong>Reframing learning as subtraction.</strong><br />
      Instead of “accumulating signal,” the paper shows that training
      subtracts structure—removing degrees of freedom until only the
      resistant manifold remains. This reverses the usual
      “capacity‑growth” intuition. <span
      data-custom-style="Hyperlink"></span></p></li>
      <li><p><strong>Optimization as manifold navigation, not
      line‑following.</strong><br />
      Overparameterization means there is no unique optimum—only a
      degenerate <strong>solution set</strong>. The protocol is derived
      from navigating that set as geometry evolves under erosion and
      friction. <span data-custom-style="Hyperlink"></span></p></li>
      <li><p><strong>Actionable diagnostics.</strong><br />
      Intrinsic dimension collapse, Fisher‑rank plateauing, and
      friction‑field shifts provide measurable triggers for phase
      transitions. This gives the framework operational value beyond
      theory. <span data-custom-style="Hyperlink"></span></p></li>
      </ul>
      <hr />
      <p><strong>4) Theoretical Implications <em>(Assuming the Work Is
      Correct)</em></strong></p>
      <ul>
      <li><p><strong>Overparameterization is beneficial because it
      creates “space to erode.”</strong><br />
      The model predicts that generalization improves when the network
      begins with <em>more redundant structure</em>, not less—providing
      richer negative space for gradient erosion to carve. <span
      data-custom-style="Hyperlink"></span></p></li>
      <li><p><strong>Fractality and hyperbolicity are emergent, not
      architectural.</strong><br />
      Multiscale roughness comes from <strong>layered erosion</strong>,
      and hyperbolic curvature emerges from <strong>exponentially rare
      voids</strong> created during carving—not from a choice of
      architecture. <span data-custom-style="Hyperlink"></span></p></li>
      <li><p><strong>Intrinsic dimension must collapse during effective
      training.</strong><br />
      The lattice predicts that good training runs always show an
      early‑to‑mid‑phase drop in intrinsic dimension—observed
      empirically in grokking, transformers, and quadratic bowls. <span
      data-custom-style="Hyperlink"></span></p></li>
      <li><p><strong>Optimizer perspective must be re‑asked once the
      manifold sharpens.</strong><br />
      Natural gradient–style methods (or Fisher‑aware schedulers) become
      most useful <strong>after</strong> the manifold has been carved,
      not before. This sequencing is structural, not heuristic. <span
      data-custom-style="Hyperlink"></span></p></li>
      </ul>
      <hr />
      <p><strong>5) Potential Implications <em>(Downstream, Not
      Predictions)</em></strong></p>
      <p><strong>A) Training Efficiency &amp; Scaling</strong></p>
      <ul>
      <li><p>Phased schedules outperform single‑phase or two‑phase
      approaches in degenerate manifolds.</p></li>
      <li><p>Fisher‑aware recalibration at the right moment may become a
      standard primitive (similar to LR warmup).</p></li>
      <li><p>Diagnostics like intrinsic‑dimension trajectories could
      guide early stopping or prevent mode collapse.<br />
      <span data-custom-style="Hyperlink"></span></p></li>
      </ul>
      <p><strong>B) Model Design</strong></p>
      <ul>
      <li><p>Architectures that explicitly encourage degeneracy (wide
      layers, overcomplete blocks, redundant heads) may be structurally
      advantaged because they enhance the erosion process. <span
      data-custom-style="Hyperlink"></span></p></li>
      <li><p>Future work may design architectures by shaping the initial
      manifold so erosion converges faster. <span
      data-custom-style="Hyperlink"></span></p></li>
      </ul>
      <p><strong>C) Theory of Generalization</strong></p>
      <ul>
      <li><p>Generalization emerges as a geometric outcome of landing in
      low‑friction valleys carved by erosion, not from explicit
      regularization or compression.</p></li>
      <li><p>This relates grokking, mode connectivity, intrinsic
      dimension, and Fisher spectra to a single cause. <span
      data-custom-style="Hyperlink"></span></p></li>
      </ul>
      <p><strong>D) Tooling &amp; Infrastructure</strong></p>
      <ul>
      <li><p>Training libraries may integrate real‑time Fisher‑rank and
      intrinsic‑dimension monitors as first‑class citizens.</p></li>
      <li><p>Auto‑phase optimizers could replace hand‑crafted LR
      schedules. <span data-custom-style="Hyperlink"></span></p></li>
      </ul>
  </div>

   
   

  <footer>
    CC BY 4.0 | DOI: 10.5281/zenodo.18489279 | www.3pilgrim.com
  </footer>
</body>
</html>